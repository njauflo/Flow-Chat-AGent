{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNx4Xkqiq+ZvXV9PvD9OgcV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97b93e3a983841578de325b5844d667e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cbb5836f71a41d2a15421feab2228d7",
              "IPY_MODEL_96b0060f06ce489f9cafeab939613a79",
              "IPY_MODEL_bcbd8b5e2c1c4dceb3008d3f5675e4dd"
            ],
            "layout": "IPY_MODEL_c45ff9a5a9db4c71bf94081a1c747505"
          }
        },
        "2cbb5836f71a41d2a15421feab2228d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62271f26549b411a9561d9003e8e9979",
            "placeholder": "​",
            "style": "IPY_MODEL_77bf4ec702a74ff091f392ba238cd4f7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "96b0060f06ce489f9cafeab939613a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f1b2e6f74c4973ad77025c58e2047a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_125d0cc9419c43e78bf948d67e2bdec4",
            "value": 2
          }
        },
        "bcbd8b5e2c1c4dceb3008d3f5675e4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7e368b16c4246bf9b2eec1fdcbede8a",
            "placeholder": "​",
            "style": "IPY_MODEL_027933fafe4d46138874a0fb1b046eb9",
            "value": " 2/2 [00:01&lt;00:00,  1.83it/s]"
          }
        },
        "c45ff9a5a9db4c71bf94081a1c747505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62271f26549b411a9561d9003e8e9979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77bf4ec702a74ff091f392ba238cd4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7f1b2e6f74c4973ad77025c58e2047a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125d0cc9419c43e78bf948d67e2bdec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7e368b16c4246bf9b2eec1fdcbede8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "027933fafe4d46138874a0fb1b046eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njauflo/Flow-Chat-Agent/blob/main/Flow_Chat_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flow Chat Agent\n",
        "This project builds a chatbot powered by the Llama 2 language model from NousResearch. It utilizes a sentiment analysis model from huggingface to tailor responses based on user input.\n",
        "\n",
        "For known questions, the chatbot retrieves answers from a user-defined QA dataset stored in a CSV file. If the question isn't found, the sentiment model determines the user's emotional tone. Llama 2 then generates a creative response, and the new question-answer pair gets added to the dataset for future reference.\n",
        "\n",
        "This approach allows the chatbot to learn and improve over time, offering both pre-defined answers and dynamic responses based on user sentiment."
      ],
      "metadata": {
        "id": "OS8pTOKbfwrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmvop5F2mLa2",
        "outputId": "8fb44be9-9196-453b-fc77-943b3b6d30e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "n7Lp-l5jmb3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyPbigZ-me1T",
        "outputId": "cc931f5a-b42b-4ac4-8106-ed67ffcdd67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file =  'qa_dataset.csv'\n",
        "\n",
        "if not os.path.exists(csv_file):\n",
        "  qa_data = {\n",
        "      'question': [\"What is the name of Julius Magellan's dog?\", \"Who is Julius Magellan's dog?\"],\n",
        "      'answer': [\"The name of Julius Magellan's dog is Sparky\",\" Julius Magellan's dog is called Sparky\"]\n",
        "  }\n",
        "  qa_df = pd.DataFrame(qa_data)\n",
        "  qa_df.to_csv(csv_file, index=False)\n",
        "else:\n",
        "  qa_df = pd.read_csv(csv_file)\n"
      ],
      "metadata": {
        "id": "_tr9P86LmgO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = pd.read_csv(csv_file)\n",
        "print(qa_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZb7D8Esml7z",
        "outputId": "016f4306-dfb4-4a78-edce-4b4626818db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     question  \\\n",
            "0  What is the name of Julius Magellan's dog?   \n",
            "1               Who is Julius Magellan's dog?   \n",
            "\n",
            "                                        answer  \n",
            "0  The name of Julius Magellan's dog is Sparky  \n",
            "1       Julius Magellan's dog is called Sparky  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "# Load sentiment analysis model\n",
        "sentiment_model_id = \"siebert/sentiment-roberta-large-english\"\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_id)\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_id)\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=2048,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "97b93e3a983841578de325b5844d667e",
            "2cbb5836f71a41d2a15421feab2228d7",
            "96b0060f06ce489f9cafeab939613a79",
            "bcbd8b5e2c1c4dceb3008d3f5675e4dd",
            "c45ff9a5a9db4c71bf94081a1c747505",
            "62271f26549b411a9561d9003e8e9979",
            "77bf4ec702a74ff091f392ba238cd4f7",
            "f7f1b2e6f74c4973ad77025c58e2047a",
            "125d0cc9419c43e78bf948d67e2bdec4",
            "b7e368b16c4246bf9b2eec1fdcbede8a",
            "027933fafe4d46138874a0fb1b046eb9"
          ]
        },
        "id": "LOC8cmRwmrzj",
        "outputId": "db598055-0a92-4635-cda6-be8a1118f1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97b93e3a983841578de325b5844d667e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    global qa_df\n",
        "\n",
        "    # Analyze sentiment\n",
        "    inputs = sentiment_tokenizer(question, return_tensors=\"pt\")\n",
        "    outputs = sentiment_model(**inputs)\n",
        "    predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n",
        "\n",
        "    if not answer.empty:\n",
        "        print(f\"Answer from QA dataset: {answer.iloc[0]}\")\n",
        "    else:\n",
        "        if predicted_label == 0:  # Negative sentiment\n",
        "            response = \"I'm sorry to hear that. Would you like to talk about it?\"\n",
        "        elif predicted_label == 1:  # Positive sentiment\n",
        "            response = \"That's great to hear! How can I help you today?\"\n",
        "        else:  # Neutral sentiment\n",
        "            response = \"I'm here to listen. What's on your mind?\"\n",
        "\n",
        "        response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']\n",
        "        response = response.replace(f\"Answer: {question}\", \"\").strip()\n",
        "        print(f\"Answer from Llama 2: {response}\")\n",
        "\n",
        "        if not any(qa_df['question'].str.lower() == question.lower()):\n",
        "            new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n",
        "            qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
        "            qa_df.to_csv(csv_file, index=False)\n",
        "            print(\"New QA pair added to the dataset.\")"
      ],
      "metadata": {
        "id": "54Za6dRVmv_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_1 = \"I'm feeling overwhelmed and anxious because I have too many deadlines approaching. What coping mechanisms can you suggest?\"\n",
        "answer_question(question_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyfw24oNm4-Q",
        "outputId": "d34a63f1-1d63-4e2a-c928-e8d70f9fc2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from QA dataset: I'm feeling overwhelmed and anxious because I have too many deadlines approaching. What coping mechanisms can you suggest?\n",
            "Dear Overwhelmed,\n",
            "I completely understand how you're feeling. Having too many deadlines approaching can be stressful and anxiety-provoking. Here are some coping mechanisms that may help you manage your workload and reduce your stress levels:\n",
            "\n",
            "1. Prioritize your tasks: Make a list of all your deadlines and prioritize them based on their importance and urgency. Focus on completing the most critical tasks first, and then work your way down the list.\n",
            "2. Break tasks into smaller chunks: Large projects or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_2 = \"I've been struggling with thoughts of self-harm. Can you help me understand why I'm feeling this way?\"\n",
        "answer_question(question_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emYdExYem7jI",
        "outputId": "906aba94-3530-4cda-cb15-a86c7fa79e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from Llama 2: I've been struggling with thoughts of self-harm. Can you help me understand why I'm feeling this way?\n",
            "Self-harm is a complex and sensitive topic, and I'm here to help you in any way I can. It's important to understand that self-harm is not a moral failing or a personal weakness, but rather a coping mechanism that some people use to deal with overwhelming emotions or situations.\n",
            "\n",
            "There are many reasons why someone might turn to self-harm, including:\n",
            "\n",
            "1. Emotional pain: Self-harm can be a way to temporarily distract oneself from emotional pain or discomfort. It can provide a sense of relief\n",
            "New QA pair added to the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_3 = \"You mentioned break tasks into smaller chunks as a coping strategy. Can you explain what it is and how to practice it?\"\n",
        "answer_question(question_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8XDhISRm-65",
        "outputId": "14d94279-cac6-4efa-8bfe-2a90b9123b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from Llama 2: You mentioned break tasks into smaller chunks as a coping strategy. Can you explain what it is and how to practice it?\n",
            "\n",
            "Sure! Break tasks into smaller chunks is a coping strategy that involves dividing a large or overwhelming task into smaller, more manageable parts. This can help make the task feel less daunting and more achievable, which can reduce stress and anxiety.\n",
            "\n",
            "Here are some steps you can follow to practice breaking tasks into smaller chunks:\n",
            "\n",
            "1. Identify the task you want to break down: Think about the task you want to tackle and identify its main components or steps.\n",
            "2. Divide the task into smaller parts: Break each step\n",
            "New QA pair added to the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eo82zBcm_uy",
        "outputId": "a217d1bd-0434-4bdd-a953-1b51bf450e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            question  \\\n",
            "0         What is the name of Julius Magellan's dog?   \n",
            "1                      Who is Julius Magellan's dog?   \n",
            "2  I'm feeling overwhelmed and anxious because I ...   \n",
            "3  I've been struggling with thoughts of self-har...   \n",
            "4  You mentioned break tasks into smaller chunks ...   \n",
            "\n",
            "                                              answer  \n",
            "0        The name of Julius Magellan's dog is Sparky  \n",
            "1             Julius Magellan's dog is called Sparky  \n",
            "2  I'm feeling overwhelmed and anxious because I ...  \n",
            "3  I've been struggling with thoughts of self-har...  \n",
            "4  You mentioned break tasks into smaller chunks ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3luIiN4qTR1",
        "outputId": "0c1817a4-91af-41b0-dd30-cc3e1ed53bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_chat_interface(question):\n",
        "  global qa_df\n",
        "  answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n",
        "\n",
        "  if not answer.empty:\n",
        "    return f\"Answer from QA dataset: {answer.iloc[0]}\"\n",
        "  else:\n",
        "    response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']\n",
        "    response = response.replace(f\"Answer: {question}\", \"\").strip()\n",
        "\n",
        "    if not any(qa_df['question'].str.lower() == question.lower()):\n",
        "      new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n",
        "      qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
        "      qa_df.to_csv(csv_file, index=False)\n",
        "      return f\"Answer from Llama 2: {response} \\n(New QA pair added to the datatset)\""
      ],
      "metadata": {
        "id": "zlQZc0EQqbkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(\n",
        "    fn=gradio_chat_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Llama 2 Chatbot with QA Pipeline\",\n",
        "    description=\"Ask a question and the chatbot will respond using a pre-defined QA dataset or Llama 2 if the answer is not in the dataset.\"\n",
        ")"
      ],
      "metadata": {
        "id": "IyXFjcewr0bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "MaNk7LdAtfgl",
        "outputId": "b6cbc6ad-8f56-45fe-e340-b0a7d0a738e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6d9b31effc32f9c633.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6d9b31effc32f9c633.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}